{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "StPQRgtZ-KDc"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oMiEEqR3s1Ok",
    "outputId": "5979d15e-d722-4263-f999-55efc873ee20"
   },
   "outputs": [],
   "source": [
    "# !pip install NERDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XDLnc7Mt_Apk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hl973/.local/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from NERDA.models import NERDA\n",
    "import torch\n",
    "import pickle\n",
    "import sklearn\n",
    "import random\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EiwQRlAKtEpC",
    "outputId": "60d4571e-ead0-43d1-ee28-606799e49d06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading https://data.deepai.org/conll2003.zip\n"
     ]
    }
   ],
   "source": [
    "# from NERDA.datasets import get_conll_data, download_conll_data \n",
    "# download_conll_data()\n",
    "# training = get_conll_data('train')\n",
    "# validation = get_conll_data('valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HZNJ22anzSdM",
    "outputId": "689159c0-c017-44af-fa61-fc49536d7bb8"
   },
   "outputs": [],
   "source": [
    "# print(training.keys())\n",
    "# print(training['sentences'])\n",
    "# print(training['tags'])\n",
    "# print(np.array(training['sentences']).shape)\n",
    "# print(len((training['tags'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "NRgDLeC08zK7"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertConfig, BertTokenizer, BertModel\n",
    "\n",
    "model_dir = \"bert-base-chinese\"\n",
    "\n",
    "config = BertConfig.from_pretrained(model_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "model = BertModel.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fQRYP5Y-6jT",
    "outputId": "119282d3-1ba3-4fae-9a08-62c7f513982d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_bert.BertModel'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "deRkcAle9okv",
    "outputId": "fc1e5904-36cf-4adf-c375-79e164220e9a"
   },
   "outputs": [],
   "source": [
    "# data in result.json is the labeled ming data\n",
    "# with open('/home/aistudio/result.json') as jf:\n",
    "#     ming=json.load(jf)\n",
    "with open('result.json') as jf:\n",
    "    ming=json.load(jf)\n",
    "\n",
    "#unpack .json file, wash the data and transfer them into ndarray\n",
    "# x_data, y_data corresponds to texts and tags\n",
    "def dataGen(ming):\n",
    "    person_ids=np.array(list(ming.keys()))\n",
    "    person_ids.sort()\n",
    "\n",
    "    #idNum=len(person_ids)\n",
    "\n",
    "    x_data=[]\n",
    "    y_data=[]\n",
    "\n",
    "    indexer=0\n",
    "    for person_id in person_ids:\n",
    "        \n",
    "        char_tag=ming[person_id]['char_tag']\n",
    "        x_data.append([])\n",
    "        y_data.append([])\n",
    "        omit_len=len(person_id)\n",
    "        for i in range(omit_len+1,len(char_tag)):\n",
    "            x_data[indexer].append(char_tag[i][0])\n",
    "            y_data[indexer].append(char_tag[i][1])\n",
    "         \n",
    "        indexer=indexer+1\n",
    "\n",
    "    for i in range(0,len(y_data)):\n",
    "        for j in range(0,len(y_data[i])):\n",
    "            old_text=y_data[i][j]\n",
    "            #convert labels like 'B_date_reign' into 'B-date-reign'\n",
    "            new_text=old_text.replace(\"_\",\"-\")\n",
    "            y_data[i][j]=new_text\n",
    "\n",
    "\n",
    "    return x_data,y_data,person_ids\n",
    "\n",
    "\n",
    "#construct train,validate and test set\n",
    "#train_set_rate indicates the proportion of trainning data\n",
    "#validate_set_rate indicates the proportion of validation data\n",
    "def splitTrain(x_data,y_data,person_ids,train_set_rate,validate_set_rate):\n",
    "    x_data=np.array(x_data)\n",
    "    y_data=np.array(y_data)\n",
    "    \n",
    "    temp=np.array([x_data,y_data])\n",
    "    temp=temp.T\n",
    "    \n",
    "    ming_data=pd.DataFrame(temp,index=person_ids,columns=['text_a','label'])\n",
    "\n",
    "    #To save data as .tsv files, we must separate the characters using \\002 marker\n",
    "    for i in range(0,len(ming_data['text_a'])):\n",
    "        #print(i)\n",
    "        ming_data['text_a'][i]='\\002'.join(ming_data['text_a'][i])\n",
    "        ming_data['label'][i]='\\002'.join(ming_data['label'][i])\n",
    "        ##ming_data['text_a'][i]+='\\002'\n",
    "        ##ming_data['label'][i]+='\\002'\n",
    "        #ming_data['text_a'][i]=str(ming_data['text_a'][i])\n",
    "        #ming_data['label'][i]=str(ming_data['label'][i])\n",
    "        \n",
    "        \n",
    "\n",
    "    np.random.seed(int(time.time()))\n",
    "    ming_data=ming_data.sample(frac=1.0)\n",
    "    \n",
    "    idNum=len(person_ids)\n",
    "    train_size=int(np.floor(idNum*train_set_rate))\n",
    "    validate_size=int(np.floor(idNum*validate_set_rate))\n",
    "\n",
    "    train_set=ming_data[0:train_size]\n",
    "\n",
    "    validate_set=ming_data[train_size:train_size+validate_size]\n",
    "    test_set=ming_data[train_size+validate_size:idNum]\n",
    "    return train_set,validate_set,test_set,ming_data\n",
    "\n",
    "\n",
    "#Build the first model: 50% of ming as train and 20% of ming as test\n",
    "x_data,y_data,person_ids=dataGen(ming)\n",
    "t,v,testing,ming_data=splitTrain(x_data,y_data,person_ids,0.7,0.15)\n",
    "\n",
    "#generate x,y of train,validate and test\n",
    "x_train=np.array(t['text_a'])\n",
    "y_train=np.array(t['label'])\n",
    "x_validate=np.array(v['text_a'])\n",
    "y_validate=np.array(v['label'])\n",
    "\n",
    "x_test=np.array(testing['text_a'])\n",
    "y_test=np.array(testing['label'])\n",
    "\n",
    "# We can save them as .csv files if we want\n",
    "t.to_csv('train.csv')\n",
    "v.to_csv('validate.csv')\n",
    "\n",
    "# Acquire the length of training set\n",
    "train_len=[]\n",
    "for i in range(0,len(x_train)):\n",
    "    train_len.append(len(x_train[i]))\n",
    "\n",
    "max_len=max(train_len)\n",
    "\n",
    "# The tag list, which specifies how many tags are of interest.\n",
    "tag_list = tag_list = [\"O\",\n",
    "        \"B_date_reign\", \"I_date_reign\",\n",
    "        \"B_date_year\", \"I_date_year\",\n",
    "        \"B_office_voa\", \"I_office_voa\",\n",
    "        \"B_office_title\", \"I_office_title\",\n",
    "        \"B_place_placename\", \"I_place_placename\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "MPHvpDpZpn8x"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Q4VNqq7IIsQl"
   },
   "outputs": [],
   "source": [
    "sets = [x_train, x_validate, y_train, y_validate, x_test, y_test]\n",
    "final_sets = [] \n",
    "for group in sets:\n",
    "    data_lst = []\n",
    "    for sent_or_label in group:\n",
    "        tokenized = sent_or_label.split('\\x02')\n",
    "        if len(tokenized) > MAX_LENGTH:\n",
    "            tokenized = sent_or_label.split('\\x02')[:MAX_LENGTH]\n",
    "            data_lst.append(tokenized)\n",
    "        elif (len(tokenized) > 1 or tokenized[0] != ''):\n",
    "            data_lst.append(tokenized)\n",
    "        else:\n",
    "          # Do not include\n",
    "          pass\n",
    "    final_sets.append(data_lst)\n",
    "\n",
    "train_sent, valid_sent, train_labels, valid_labels, test_sent, test_labels = final_sets[0], final_sets[1], final_sets[2], final_sets[3], final_sets[4], final_sets[5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1330\n",
      "1328\n",
      "6200\n"
     ]
    }
   ],
   "source": [
    "print(len(x_test))\n",
    "print(len(x_validate))\n",
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_n3sunbdFPQU"
   },
   "outputs": [],
   "source": [
    "training_dict = {'sentences': train_sent, 'tags': train_labels}\n",
    "validation_dict = {'sentences': valid_sent, 'tags': valid_labels}\n",
    "test_dict = {'sentences': test_sent, 'tags':test_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6196"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_dict['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(test_dict, open('test_dict.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cIqMJmRBQdA9",
    "outputId": "572476ca-b2b8-45fd-a503-586e1a1002f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6196,)\n",
      "['府', '學', '生', '。', '膂', '力', '過', '人', '，', '嘗', '手', '搏', '猛', '虎', '斃', '之', '。', '應', '試', '場', '屋', '時', '，', '會', '火', '起', '，', '以', '手', '救', '活', '者', '幾', '千', '人', '。', '後', '為', '左', '太', '中', '大', '夫', '，', '會', '盜', '起', '掠', '庫', '，', '文', '偉', '將', '兵', '追', '及', '之', '，', '盜', '伏', '地', '請', '死', '，', '文', '偉', '命', '各', '杖', '三', '十', '，', '令', '自', '新', '，', '部', '御', '史', '竟', '以', '縱', '盜', '劾', '之', '，', '免', '官', '。']\n",
      "6196\n",
      "89\n",
      "1328\n"
     ]
    }
   ],
   "source": [
    "print(np.array(train_sent).shape)\n",
    "print((train_sent[0]))\n",
    "print(len(train_labels))\n",
    "print(len(train_labels[0]))\n",
    "print(len(valid_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "3UDxhd9A5ZjO"
   },
   "outputs": [],
   "source": [
    "# hyperparameters for network\n",
    "dropout = 0.1\n",
    "# hyperparameters for training\n",
    "training_hyperparameters = {\n",
    "'epochs' : 4,\n",
    "'warmup_steps' : 200,\n",
    "'train_batch_size': 13,\n",
    "'learning_rate': 0.0001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_mwjzKIM0BxB",
    "outputId": "5f9ccb4d-c3e8-4234-9fbf-cb9bc23a81ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device automatically set to: cpu\n"
     ]
    }
   ],
   "source": [
    "model = NERDA(\n",
    "dataset_training = training_dict,\n",
    "dataset_validation = validation_dict,\n",
    "tag_scheme = [\n",
    "        \"B-date-reign\", \"I-date-reign\",\n",
    "        \"B-date-year\", \"I-date-year\",\n",
    "        \"B-office-voa\", \"I-office-voa\",\n",
    "        \"B-office-title\", \"I-office-title\",\n",
    "        \"B-place-placename\", \"I-place-placename\"], \n",
    "tag_outside = 'O',\n",
    "transformer = model_dir,\n",
    "dropout = dropout,\n",
    "hyperparameters = training_hyperparameters,\n",
    "max_len = 512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "teQl_LuvHIfV",
    "outputId": "63e543a7-a258-4c3f-eeb1-2d9d3f708141"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/511 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 60/511 [29:27<3:41:29, 29.47s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-c72315b99576>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/NERDA/models.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m                                                         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                                                         \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                                                         **self.hyperparameters)\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m# attach as attributes to class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/NERDA/training.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(network, tag_encoder, tag_outside, transformer_tokenizer, transformer_config, dataset_training, dataset_validation, max_len, train_batch_size, validation_batch_size, epochs, warmup_steps, learning_rate, device, fixed_seed, num_workers)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n Epoch {:} / {:}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_validate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/NERDA/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, optimizer, device, scheduler, n_tags)\u001b[0m\n\u001b[1;32m     22\u001b[0m                             \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                             n_tags)\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "UZby9xBc_eGR"
   },
   "outputs": [],
   "source": [
    "# path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "_hfUgZvS_cVk"
   },
   "outputs": [],
   "source": [
    "# # Part of Task1: save the data into tsv\n",
    "# # t-> train, v-> validate, testing -> test\n",
    "# # ming_data -> whole dataset, also saved in case it might be of use\n",
    "# t.to_csv((path + 'train.tsv'),sep='\\t',columns=['text_a','label'],encoding='utf_8_sig',index=None)\n",
    "# v.to_csv((path + 'validate.tsv'),sep='\\t',columns=['text_a','label'],encoding='utf_8_sig',index=None)\n",
    "# testing.to_csv((path + 'testing.tsv'),sep='\\t',columns=['text_a','label'],encoding='utf_8_sig',index=None)\n",
    "# ming_data.to_csv((path + 'dataset.tsv'),sep='\\t',columns=['text_a','label'],encoding='utf_8_sig',index=None)\n",
    "\n",
    "# # predict_data is the Jin biographical data without tags\n",
    "# predict_data=pd.read_table('test_data.txt')\n",
    "\n",
    "# # 'content_without_name' consists of characters in the texts\n",
    "# text=predict_data['content_without_name']\n",
    "\n",
    "# # Also save it as .tsv files in case\n",
    "# text.to_csv((path + 'predict.tsv'),encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Weights for network loaded from NERDA_model.bin'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_network_from_file(model_path='NERDA_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['字',\n",
       " '鳴',\n",
       " '玉',\n",
       " '，',\n",
       " '耒',\n",
       " '陽',\n",
       " '人',\n",
       " '。',\n",
       " '諸',\n",
       " '生',\n",
       " '，',\n",
       " '天',\n",
       " '會',\n",
       " '十',\n",
       " '六',\n",
       " '年',\n",
       " '闖',\n",
       " '賊',\n",
       " '犯',\n",
       " '耒',\n",
       " '陽',\n",
       " '，',\n",
       " '如',\n",
       " '珂',\n",
       " '拒',\n",
       " '戰',\n",
       " '死',\n",
       " '之',\n",
       " '。']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sent[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluate_performance(validation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['字',\n",
       " '一',\n",
       " '菴',\n",
       " '，',\n",
       " '號',\n",
       " '退',\n",
       " '翁',\n",
       " '，',\n",
       " '上',\n",
       " '虞',\n",
       " '孫',\n",
       " '氏',\n",
       " '子',\n",
       " '。',\n",
       " '年',\n",
       " '十',\n",
       " '三',\n",
       " '為',\n",
       " '僧',\n",
       " '，',\n",
       " '居',\n",
       " '杭',\n",
       " '州',\n",
       " '上',\n",
       " '天',\n",
       " '竺',\n",
       " '寺',\n",
       " '。',\n",
       " '興',\n",
       " '定',\n",
       " '中',\n",
       " '奉',\n",
       " '命',\n",
       " '輯',\n",
       " '禪',\n",
       " '宗',\n",
       " '語',\n",
       " '錄',\n",
       " '，',\n",
       " '後',\n",
       " '又',\n",
       " '輯',\n",
       " '法',\n",
       " '華',\n",
       " '科',\n",
       " '注',\n",
       " '，',\n",
       " '大',\n",
       " '明',\n",
       " '法',\n",
       " '數',\n",
       " '等',\n",
       " '書',\n",
       " '，',\n",
       " '屢',\n",
       " '授',\n",
       " '鼻',\n",
       " '骨',\n",
       " '德',\n",
       " '部',\n",
       " '都',\n",
       " '監',\n",
       " '，',\n",
       " '年',\n",
       " '七',\n",
       " '十',\n",
       " '四',\n",
       " '卒',\n",
       " '。']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1_test = pd.read_csv('group1.csv', dtype={'id': np.int32, 'bio': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'bio'], dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_1_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hl973/.local/lib/python3.7/site-packages/NERDA/preprocessing.py:76: UserWarning: Sentence #0 length 1067 exceeds max_len 512 and has been truncated\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hl973/.local/lib/python3.7/site-packages/NERDA/preprocessing.py:76: UserWarning: Sentence #0 length 1655 exceeds max_len 512 and has been truncated\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hl973/.local/lib/python3.7/site-packages/NERDA/preprocessing.py:76: UserWarning: Sentence #0 length 788 exceeds max_len 512 and has been truncated\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hl973/.local/lib/python3.7/site-packages/NERDA/preprocessing.py:76: UserWarning: Sentence #0 length 1942 exceeds max_len 512 and has been truncated\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hl973/.local/lib/python3.7/site-packages/NERDA/preprocessing.py:76: UserWarning: Sentence #0 length 1834 exceeds max_len 512 and has been truncated\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hl973/.local/lib/python3.7/site-packages/NERDA/preprocessing.py:76: UserWarning: Sentence #0 length 1004 exceeds max_len 512 and has been truncated\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hl973/.local/lib/python3.7/site-packages/NERDA/preprocessing.py:76: UserWarning: Sentence #0 length 566 exceeds max_len 512 and has been truncated\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hl973/.local/lib/python3.7/site-packages/NERDA/preprocessing.py:76: UserWarning: Sentence #0 length 1779 exceeds max_len 512 and has been truncated\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hl973/.local/lib/python3.7/site-packages/NERDA/preprocessing.py:76: UserWarning: Sentence #0 length 907 exceeds max_len 512 and has been truncated\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "for idx in range(len(group_1_test)):\n",
    "    paragraph = list(group_1_test.iloc[idx]['bio'])\n",
    "    preds = model.predict([paragraph])\n",
    "    padded_paragraph = paragraph.copy()\n",
    "    padded_preds = preds.copy()[0]\n",
    "    if len(paragraph) < 512:\n",
    "        padded_paragraph += [0 for i in range(512 - len(paragraph))]\n",
    "    else:\n",
    "        padded_paragraph = padded_paragraph[:512]\n",
    "    if len(preds[0]) < 512:\n",
    "        padded_preds += [0 for i in range(512 - len(preds[0]))]\n",
    "        print(len(preds[0]))\n",
    "    assert len(padded_paragraph) == 512\n",
    "    assert len(padded_preds) == 512\n",
    "        \n",
    "    rows.append(padded_paragraph)\n",
    "    rows.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows, columns=['text/prediction']*512)\n",
    "df.to_csv('group1_results_formatted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>字禹學，曹州人。倜儻多權略。自為諸生，即習武事。弘治十五年進士。除刑部主事。有重囚越獄，人莫...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                                bio\n",
       "0   1  字禹學，曹州人。倜儻多權略。自為諸生，即習武事。弘治十五年進士。除刑部主事。有重囚越獄，人莫..."
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_1_test[group_1_test['id'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "testzero_tok = group_1_test[group_1_test['id'] == 1]['bio'][0].split(\"。\")\n",
    "testzero_tok = [list(sent) for sent in testzero if len(sent) != 0]\n",
    "test_zero = group_1_test[group_1_test['id'] == 1]['bio'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_after_tokenizing = model.predict(testzero_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hl973/.local/lib/python3.7/site-packages/NERDA/preprocessing.py:76: UserWarning: Sentence #0 length 1067 exceeds max_len 512 and has been truncated\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "results1 = model.predict([list(test_zero)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-place-placename',\n",
       "  'I-place-placename',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-office-voa',\n",
       "  'B-office-title',\n",
       "  'I-office-title',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-office-voa',\n",
       "  'B-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-date-reign',\n",
       "  'I-date-reign',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-office-voa',\n",
       "  'B-office-title',\n",
       "  'I-office-title',\n",
       "  'O',\n",
       "  'B-office-voa',\n",
       "  'B-office-title',\n",
       "  'I-office-title',\n",
       "  'O',\n",
       "  'B-office-voa',\n",
       "  'B-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'O',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-office-voa',\n",
       "  'B-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-office-voa',\n",
       "  'B-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-office-voa',\n",
       "  'B-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-office-voa',\n",
       "  'B-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-office-voa',\n",
       "  'B-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'I-office-title',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-office-voa',\n",
       "  'B-office-title',\n",
       "  'I-office-title',\n",
       "  'B-office-voa',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-office-voa',\n",
       "  'B-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'I-office-title',\n",
       "  'O']]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results1 == results_after_tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert len(x_data) == len(y_data)\n",
    "# indices = [i for i in range(len(x_data))]\n",
    "# random.shuffle(indices)\n",
    "# y_true = [y_data[i] for i in range(500)]\n",
    "# x_subset = [x_data[i] for i in range(500)]\n",
    "# y_pred = model.predict(x_subset)\n",
    "y_true = test_dict['tags'][:10]\n",
    "y_pred = model.predict(test_dict['sentences'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(y_true)\n",
    "y_true_binary = mlb.transform(y_true)\n",
    "y_pred_binary = mlb.transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.precision_score(y_true_binary, y_pred_binary, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.recall_score(y_true_binary, y_pred_binary, average='weighted' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HelenJinProject.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
